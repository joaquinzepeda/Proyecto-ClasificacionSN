{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gmbRYKD1odtB"
   },
   "source": [
    "# Tarea 1: Perceptrón Multicapa\n",
    "### EL4106 Inteligencia Computacional\n",
    "\n",
    "Profesor de Cátedra: Pablo Estévez<br>\n",
    "Profesor Auxiliar: Ignacio Reyes<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gr3dCq8pYd_w"
   },
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "colab_type": "code",
    "id": "Wiv1sT9tpZO-",
    "outputId": "a0bd7f05-2f40-44aa-bd7c-90a26898e458"
   },
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "import itertools\n",
    "import warnings\n",
    "from sklearn.metrics import confusion_matrix as sk_conf_mat\n",
    "\n",
    "# Se imprime la version de Tensorflow\n",
    "print('Tensorflow version', tf.__version__)\n",
    "# Se imprime si es que se esta utilizando GPU o no\n",
    "device_name = tf.test.gpu_device_name()\n",
    "if device_name != '/device:GPU:0':\n",
    "    warnings.warn('GPU device not found')\n",
    "else:\n",
    "    print('Found GPU at: {}'.format(device_name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lAqEQZkkRgTJ"
   },
   "source": [
    "## Preparación de la base de datos MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WwD-4ePxRuzZ"
   },
   "outputs": [],
   "source": [
    "# ----- Digito a identificar.\n",
    "chosen_digit = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "RahVklIFRfbE",
    "outputId": "962ab9ec-8076-43d1-a599-6ae3bcc18c24"
   },
   "outputs": [],
   "source": [
    "def process_dataset(images, labels, selected_class):\n",
    "    \"\"\"Revuelve datos y selecciona subconjunto segun la clase seleccionada.\n",
    "    Aplana las imagenes para que tengan la forma de un vector de largo 28*28\n",
    "    Normaliza imagenes para que en vez de tener valores entre [0,255] tengan \n",
    "    valores entre [0,1].\n",
    "    \n",
    "    Los datos de imagenes y etiquetas son revueltos, se seleccionan aquellos\n",
    "    que coinciden con la etiqueta de la clase seleccionada, y un subconjunto\n",
    "    del mismo tamaño que el anterior es seleccionado de entre todas las demas\n",
    "    clases para obtener un problema balanceado.\n",
    "    \"\"\"\n",
    "    # labels a formato one-hot\n",
    "    aux_labels = np.zeros((labels.size, 10))\n",
    "    aux_labels[np.arange(labels.size),labels] = 1\n",
    "    labels = aux_labels\n",
    "    # aplana imagenes\n",
    "    images = np.reshape(\n",
    "        images, [images.shape[0], images.shape[1]*images.shape[2]])\n",
    "    shuffled_indexes = np.random.permutation(len(labels))  # Primer shuffle\n",
    "    images = images[shuffled_indexes]\n",
    "    labels = labels[shuffled_indexes]\n",
    "    selected_column = labels[:, selected_class]\n",
    "    selected_images_indexes = np.where(selected_column == 1)[0]\n",
    "    selected_size = len(selected_images_indexes)\n",
    "    non_selected_indexes_subset = np.where(selected_column == 0)[0][:selected_size]\n",
    "    indexes = np.concatenate(\n",
    "        (selected_images_indexes, \n",
    "        non_selected_indexes_subset),\n",
    "        axis=0)\n",
    "    # No queremos que el modelo primero vea todos los datos de una clase y\n",
    "    # despues todos los de la otra, asi que volvemos a revolver.\n",
    "    np.random.shuffle(indexes)\n",
    "    images_subset = images[indexes]\n",
    "    labels_subset = selected_column[indexes]\n",
    "    labels_subset = np.array(labels_subset, dtype=np.int32)\n",
    "    return images_subset/255.0, labels_subset\n",
    "\n",
    "\n",
    "# Carga de la base de datos MNIST\n",
    "mnist = tf.keras.datasets.mnist\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "# Reserva 5,000 muestras para validacion\n",
    "x_val = x_train[-5000:]\n",
    "y_val = y_train[-5000:]\n",
    "x_train = x_train[:-5000]\n",
    "y_train = y_train[:-5000]\n",
    "\n",
    "\n",
    "# Preprocesamiento de datos de entrenamiento, validacion y testing\n",
    "training_images, training_labels = process_dataset(\n",
    "    x_train,\n",
    "    y_train,\n",
    "    chosen_digit)\n",
    "validation_images, validation_labels = process_dataset(\n",
    "    x_val,\n",
    "    y_val,\n",
    "    chosen_digit)\n",
    "testing_images, testing_labels = process_dataset(\n",
    "    x_test,\n",
    "    y_test,\n",
    "    chosen_digit)\n",
    "print('Processing ready.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 127
    },
    "colab_type": "code",
    "id": "RLcbP5NJRyH_",
    "outputId": "22fc9be1-b92c-45b5-c2c0-a362e28af55d"
   },
   "outputs": [],
   "source": [
    "# Visualizacion de algunas imagenes de los datos de entrenamiento\n",
    "\n",
    "chosen_idx = np.random.choice(training_images.shape[0], size=6, replace=False)\n",
    "\n",
    "fig, ax = plt.subplots(1, 6, figsize=(10,5))\n",
    "fig.set_facecolor('white')\n",
    "for i, idx in enumerate(chosen_idx):\n",
    "    image = training_images[idx, :]\n",
    "    digit = training_labels[idx]\n",
    "    ax[i].imshow(image.reshape((28, 28)))\n",
    "    ax[i].set_title(\"Etiqueta: %d\" % digit)\n",
    "    ax[i].axis('off')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Q4f4FFyhpdDA"
   },
   "source": [
    "## Definición de Clasificador MLP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7Bip-9FV1_WJ"
   },
   "source": [
    "### Función de costo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CjgsGFG31-no"
   },
   "outputs": [],
   "source": [
    "def loss_fn(logits, labels, loss_function_name):\n",
    "    \"\"\"Calculo de la funcion de costo, donde se puede escoger entre 2 opciones; \n",
    "    xentropy o mse.\n",
    "    \n",
    "    Se aplica el loss 'loss_function_name' entre los labels reales y la salida\n",
    "    de la MLP. Ademas, se calcula el accuracy.\n",
    "    \n",
    "    Args:\n",
    "        logits: Tensor de dimensiones (batch_size, n_classes) con los logits\n",
    "        de la salida de la MLP\n",
    "        labels: Tensor de dimensiones (batch_size,) con las etiquetas reales.\n",
    "        loss_function_name: 'cross_entropy' o 'mse', selecciona el costo.\n",
    "    \n",
    "    Returns:\n",
    "        loss: Tensor escalar que corresponde al costo calculado.\n",
    "        accuracy: Tensor escalar que corresponde al accuracy calculado.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Codificacion 'one hot' para las etiquetas de clase\n",
    "    n_classes = logits.shape[1]\n",
    "    one_hot_labels = tf.one_hot(labels, n_classes)\n",
    "    with tf.name_scope('loss'):\n",
    "        if loss_function_name == 'cross_entropy':\n",
    "            # Cross Entropy loss\n",
    "            loss = tf.reduce_mean(\n",
    "                tf.nn.softmax_cross_entropy_with_logits(\n",
    "                    logits=logits,\n",
    "                    labels=one_hot_labels),\n",
    "                name='xentropy'\n",
    "            )\n",
    "        elif loss_function_name == 'mse':\n",
    "            # Mean Squared Error loss\n",
    "            probabilities = tf.nn.softmax(logits)\n",
    "            loss = tf.reduce_mean(\n",
    "                tf.square(one_hot_labels - probabilities),\n",
    "                name='mse'\n",
    "            )\n",
    "        else:\n",
    "            raise ValueError('Wrong value for loss_function_name')\n",
    "    with tf.name_scope('accuracy'):\n",
    "        predictions = tf.argmax(logits, axis=1, output_type=tf.int32)\n",
    "        correct_predictions = tf.equal(labels, predictions)\n",
    "        accuracy = tf.reduce_mean(\n",
    "            tf.cast(correct_predictions, tf.float32),\n",
    "            name='accuracy')\n",
    "    return loss, accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Mx8DACFS8TxR"
   },
   "source": [
    "### Clase de Clasificador MLP\n",
    "Esta clase utiliza la funcion anteriores para entrenar el modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fITl9UaSydAf"
   },
   "outputs": [],
   "source": [
    "class MLPClassifier(tf.keras.Model):\n",
    "    \"\"\"Implementacion de clasificador Perceptron Multicapa.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        n_features,\n",
    "        layer_sizes,\n",
    "        loss_function_name='cross_entropy',\n",
    "        learning_rate=0.1,\n",
    "        batch_size=32,\n",
    "        max_epochs=100,\n",
    "        early_stopping=None,\n",
    "        logdir='logs'):\n",
    "        \"\"\"Construye un clasificador Perceptron Multicapa.\n",
    "        \n",
    "        Args:\n",
    "            n_features: Entero que indica el numero de caracteristicas de las entradas.\n",
    "            layer_sizes: Lista de enteros que indica el tamaño de cada capa de\n",
    "            neuronas. La salida de la capa i-esima posee dimensiones\n",
    "            (batch_size, layer_sizes[i]). El ultimo numero de la lista indica el\n",
    "            tamaño de la capa de salida, que debe ser igual al numero de clases.\n",
    "            loss_function_name: 'cross_entropy' o 'mse', selecciona el costo.\n",
    "                Por defecto es 'cross_entropy'.\n",
    "            learning_rate: Escalar que indica la tasa de aprendizaje. Al\n",
    "                seleccionar Adam este parametro es ignorado. Por defecto es 0.1\n",
    "            batch_size: Entero que indica el tamaño de los mini-batches para\n",
    "                el entrenamiento de la red.\n",
    "            max_epochs: Entero que indica el maximo numero de epocas de\n",
    "                entrenamiento (pasadas completas por los datos de entrada) \n",
    "            early_stopping: Indica cuantas veces las verificaciones en la\n",
    "                validacion deben indicar que el costo esta aumentando para\n",
    "                realizar una detencion temprana. Por defecto es None, lo cual\n",
    "                desactiva la detencion temprana.\n",
    "            logdir: String que indica el directorio en donde guardar los\n",
    "                archivos del entrenamiento. Por defecto es 'logs'.\n",
    "        \"\"\"\n",
    "        \n",
    "        # Inicializa atributos propios de un objeto del tipo tf.keras.Model\n",
    "        super().__init__()\n",
    "        # Agregar parametros al objeto\n",
    "        self.n_features = n_features\n",
    "        self.layer_sizes = layer_sizes\n",
    "        self.loss_function_name = loss_function_name\n",
    "        self.learning_rate = learning_rate\n",
    "        self.batch_size = batch_size\n",
    "        self.max_epochs = max_epochs\n",
    "        self.early_stopping = early_stopping\n",
    "        self.logdir = logdir\n",
    "        # Inicializa capas del modelo\n",
    "        self.layers_list = self._init_layers(layer_sizes)\n",
    "        # Crea un objeto optimizador\n",
    "        self.optimizer = tf.keras.optimizers.SGD(learning_rate=self.learning_rate)\n",
    "        \n",
    "    def _init_layers(self, layer_sizes):\n",
    "        \"\"\"Inicializa-crea los objetos de cada capa de la red MLP.\n",
    "\n",
    "        Args:\n",
    "          layer_sizes: Lista de enteros que indica el tamaño de cada capa de\n",
    "              neuronas. La salida de la capa i-esima posee dimensiones\n",
    "              (batch_size, layer_sizes[i]). El ultimo numero de la lista indica el\n",
    "              tamaño de la capa de salida, que debe ser igual al numero de clases.\n",
    "\n",
    "        Returns:\n",
    "          layers_list: Lista de objetos asociados a cada capa creada.\n",
    "        \"\"\"\n",
    "        n_layers = len(layer_sizes)\n",
    "        layers_list = []\n",
    "        # Inicializacion de capas del modelo\n",
    "        layers_list.append(tf.keras.layers.InputLayer((self.n_features)))\n",
    "        for i in range(n_layers):\n",
    "            layers_list.append(tf.keras.layers.Dense(layer_sizes[i], name='dense_%i' % (i+1)))\n",
    "            if i < n_layers - 1:\n",
    "                # Inicializacion de funcion de activacion de capa oculta\n",
    "                layers_list.append(tf.keras.layers.Activation(tf.nn.sigmoid))\n",
    "            else:\n",
    "                # Inicializacion de funcion de activacion de capa de salida\n",
    "                layers_list.append(tf.keras.layers.Activation(tf.nn.softmax))\n",
    "        return layers_list\n",
    "\n",
    "    def call(self, x, training=False, get_logits=False):\n",
    "        \"\"\"Metodo que entrega la salida del modelo, \n",
    "        define el paso forward de la red neuronal, relacionando las capas \n",
    "        de la red MLP entre si. Se define la arquitectura del modelo.\n",
    "\n",
    "        Args:\n",
    "          x: Tensor de entrada de dimensiones (batch_size, n_features).\n",
    "          training: Boolean que indica si se esta en fase de entrenamiento o no.\n",
    "            En este caso es irrelevante, pero resulta util al definir capas que \n",
    "            tienen diferentes comportamientos durante entrenamiento y evaluacion\n",
    "             como dropout o batch normalization \n",
    "          get_logits: Bolean que indica si como salida del modelo se obtienen los\n",
    "            logits o las predicciones\n",
    "\n",
    "        Returns:\n",
    "          x: Tensor de salida de la red, de dimensiones (batch_size, n_classes).\n",
    "        \"\"\"\n",
    "        for layer_index, layer in enumerate(self.layers_list):\n",
    "            # se se desean los logits, se retorna la salida del modelo sin la ultima activacion\n",
    "            if layer_index == (len(self.layers_list)-1) and get_logits==True:\n",
    "                return x \n",
    "            x = layer(x)\n",
    "        return x\n",
    "\n",
    "    @tf.function\n",
    "    def train_step(self, x_data, y_labels):\n",
    "        \"\"\"Metodo que realiza la actualizacion por gradiente.\n",
    "\n",
    "        Se aplica el algoritmo de optimizacion 'sgd' para ejecutar una\n",
    "        iteracion de minimizacion por gradiente sobre el loss del modelo.\n",
    "\n",
    "        Args:\n",
    "          x_data: datos de entrenamientos sobre los que calcular la loss.\n",
    "          y_labels: etiquetas de entrenamiento sobre las que calcular la loss.\n",
    "\n",
    "        Returns:\n",
    "          loss: el valor de la funcion de costo para x_data, y_labels.\n",
    "          accuracy: porcentaje de aciertos entre y_labels y las predicciones sobre x_data.\n",
    "        \"\"\"\n",
    "        with tf.GradientTape() as tape:\n",
    "            # training=True is only needed if there are layers with different\n",
    "            # behavior during training versus inference (e.g. Dropout).\n",
    "            logits = self.call(x_data, training=True, get_logits=True)\n",
    "            loss, accuracy = loss_fn(logits, y_labels, self.loss_function_name)\n",
    "            gradients = tape.gradient(loss, self.trainable_variables)\n",
    "        self.optimizer.apply_gradients(zip(gradients, self.trainable_variables))\n",
    "        return loss, accuracy\n",
    "\n",
    "    @tf.function\n",
    "    def eval_step(self, x_data, y_labels):\n",
    "        \"\"\"Metodo que evalua la funcion de costo y el accuracy del modelo.\n",
    "\n",
    "        Args:\n",
    "          x_data: datos sobre los que calcular la loss y accuracy.\n",
    "          y_labels: etiquetas sobre las que calcular la loss y accuracy.\n",
    "\n",
    "        Returns:\n",
    "          loss: el valor de la funcion de costo para x_data, y_labels.\n",
    "          accuracy: porcentaje de aciertos entre y_labels y las predicciones sobre x_data.\n",
    "        \"\"\"\n",
    "        logits = self.call(x_data, training=False, get_logits=True)\n",
    "        loss, accuracy = loss_fn(logits, y_labels, self.loss_function_name)\n",
    "        return loss, accuracy\n",
    "\n",
    "    @tf.function\n",
    "    def prediction_step(self, x_data):\n",
    "        \"\"\"Metodo que retorna predicciones para x_data\n",
    "\n",
    "        Args:\n",
    "          x_data: datos a predecir.\n",
    "\n",
    "        Returns:\n",
    "          predictions: predicciones para x_data\n",
    "        \"\"\"\n",
    "        predictions = self.call(x_data, training=False, get_logits=False)\n",
    "        return predictions\n",
    "\n",
    "    def write_to_train_summary(self, train_summary_writer, step, loss, accuracy):\n",
    "        \"\"\"Metodo que calcula metricas y parametros a ser monitoreados en tensorboard.\n",
    "        Monitorea histograma de parametros y metricas del modelo. Para datos de entrenamiento\n",
    "\n",
    "        Args:\n",
    "          train_summary_writer: Summary writer de entrenamiento\n",
    "          step: iteracion del entrenamiento\n",
    "          loss: valor de la funcion de costo para datos de entrenamiento\n",
    "          accuracy: accuracy para datos de entrenamiento\n",
    "        \"\"\"\n",
    "        with train_summary_writer.as_default():\n",
    "            for layer_i in self.layers_list:\n",
    "                layer_name = layer_i.name\n",
    "                if 'dense' in layer_name:\n",
    "                    weights, biases = layer_i.get_weights()\n",
    "                    tf.summary.histogram(layer_name + '_weights', weights, step=step)\n",
    "                    tf.summary.histogram(layer_name + '_biases', biases, step=step)\n",
    "            tf.summary.scalar(self.loss_function_name + '_loss', loss, step=step)  \n",
    "            tf.summary.scalar('accuracy', accuracy, step=step)  \n",
    "\n",
    "    def write_to_val_summary(self, val_summary_writer, step, loss, accuracy):\n",
    "        \"\"\"Metodo que calcula metricas a ser monitoreados en tensorboard.\n",
    "        Monitorea metricas del modelo. Para datos de validacion\n",
    "\n",
    "        Args:\n",
    "          validacion_summary_writer: Summary writer de validacion\n",
    "          step: iteracion del entrenamiento\n",
    "          loss: valor de la funcion de costo para datos de validacion\n",
    "          accuracy: accuracy para datos de validacion\n",
    "        \"\"\"\n",
    "        with val_summary_writer.as_default():\n",
    "            tf.summary.scalar(self.loss_function_name + '_loss', loss, step=step)  \n",
    "            tf.summary.scalar('accuracy', accuracy, step=step)  \n",
    "    \n",
    "        \n",
    "    def fit(self, X_train, y_train, X_val, y_val):\n",
    "        \"\"\"Entrenamiento del clasificador con los hiperparametros escogidos.\n",
    "        \n",
    "        Args:\n",
    "            X_train: Entradas del entrenamiento con dimensiones (n_ejemplos, n_features).\n",
    "            y_train: Etiquetas del entrenamiento con dimensiones (n_ejemplos,)\n",
    "            X_train: Entradas de la validacion con dimensiones (n_ejemplos, n_features).\n",
    "            y_train: Etiquetas de la validacion con dimensiones (n_ejemplos,)\n",
    "            \n",
    "        Returns:\n",
    "            train_stats: Diccionario con datos historicos del entrenamiento.\n",
    "        \"\"\"\n",
    "        # Creacion de 'writers' que guardan datos para Tensorboard\n",
    "        train_summary_writer = tf.summary.create_file_writer(\n",
    "            self.logdir + '/train')\n",
    "        val_summary_writer = tf.summary.create_file_writer(self.logdir + '/val')\n",
    "        print('\\n\\n[Beginning training of MLP at logdir \"%s\"]\\n' % (self.logdir,))    \n",
    "        # Definicion de variables utiles para el entrenamiento\n",
    "        n_batches = int(X_train.shape[0] / self.batch_size)\n",
    "        prev_validation_loss = 100.0\n",
    "        validation_period = 100\n",
    "        early_stop_flag = False\n",
    "        start_time = time.time()\n",
    "        iteration_history = []\n",
    "        train_loss_history = []\n",
    "        train_acc_history = []\n",
    "        val_loss_history = []\n",
    "        val_acc_history = []\n",
    "        # Se crea dataset para iterar sobre los batch de los datos de entreneminto. \n",
    "        # Para cada nueva epoca, se hacer un shuffle al set de train\n",
    "        train_dataset = tf.data.Dataset.from_tensor_slices(\n",
    "            (X_train, y_train)).shuffle(X_train.shape[0]).batch(self.batch_size)\n",
    "        # Ciclo que recorre una epoca completa de los datos cada vez\n",
    "        for epoch in range(self.max_epochs):\n",
    "            if early_stop_flag:\n",
    "                # Si early stopping se activo, detener el entrenamiento\n",
    "                break            \n",
    "            # Ciclo que recorre los mini batches del set de train\n",
    "            for i, (X_batch, y_batch) in enumerate(train_dataset):\n",
    "                if early_stop_flag:\n",
    "                    # Si early stopping se activo, detener el entrenamiento\n",
    "                    break  \n",
    "                iteration = epoch * n_batches + i                    \n",
    "                # Ejecutar una iteracion de gradiente\n",
    "                self.train_step(X_batch, y_batch)\n",
    "                # Obtener estadisticas del entrenamiento\n",
    "                if iteration % validation_period == 0:\n",
    "                    iteration_history.append(iteration)\n",
    "                    # Estadisticas en el set de validacion\n",
    "                    val_loss, val_acc = self.eval_step(X_val, y_val)\n",
    "                    # Escribir estadisticas de validacion en tensorboard\n",
    "                    self.write_to_val_summary(\n",
    "                        val_summary_writer, iteration, val_loss, val_acc)\n",
    "                    val_loss_history.append(val_loss)\n",
    "                    val_acc_history.append(val_acc)\n",
    "                    # Estadisticas en el set de entrenamiento\n",
    "                    train_loss, train_acc = self.eval_step(X_train, y_train)\n",
    "                    # Escribir estadisticas e histogramas de parametros de \n",
    "                    # entrenamiento en tensorboard\n",
    "                    self.write_to_train_summary(\n",
    "                        train_summary_writer, iteration, train_loss, train_acc)\n",
    "                    train_loss_history.append(train_loss)\n",
    "                    train_acc_history.append(train_acc)\n",
    "                    \n",
    "                    print('Epoch: %d/%d, iter: %d. ' %\n",
    "                          (epoch+1, self.max_epochs, iteration), end='')\n",
    "                    print('Loss (train/val): %.3f / %.3f. Val. acc: %.1f%%' %\n",
    "                          (train_loss, val_loss, val_acc * 100), end='')\n",
    "                    \n",
    "                    # Chequear condicion de early_stopping\n",
    "                    if self.early_stopping is not None:\n",
    "                        if val_loss > prev_validation_loss:\n",
    "                            validation_checks += 1\n",
    "                        else:\n",
    "                            validation_checks = 0\n",
    "                            prev_validation_loss = val_loss\n",
    "                        print(', Val. checks: %d/%d' %\n",
    "                              (validation_checks, self.early_stopping))\n",
    "                        if validation_checks >= self.early_stopping:\n",
    "                            early_stop_flag = True\n",
    "                            print('Early stopping')\n",
    "                    else:\n",
    "                        print('')\n",
    "            elap_time = time.time()-start_time\n",
    "            print(\"Epoch finished. Elapsed time %1.4f [s]\\n\" % (elap_time,))\n",
    "        # Guardar estadisticas en un diccionario\n",
    "        train_stats = {\n",
    "            'iteration_history': np.array(iteration_history),\n",
    "            'train_loss_history': np.array(train_loss_history),\n",
    "            'train_acc_history': np.array(train_acc_history),\n",
    "            'val_loss_history': np.array(val_loss_history),\n",
    "            'val_acc_history': np.array(val_acc_history)\n",
    "        }\n",
    "        # Guardar grafo de evaluacion en tensorboard\n",
    "        self.write_graph_to_summary(train_summary_writer, X_train)\n",
    "        return train_stats\n",
    "\n",
    "    def write_graph_to_summary(self, train_summary_writer, X_train):\n",
    "        \"\"\"Metodo que guarda el grafo de evaluacion (arquitectura del modelo)\n",
    "        en tensorboard. Se guardan las operaciones involugradas en un passo forward\n",
    "        o self.call del modelo\n",
    "\n",
    "        Args:\n",
    "          train_summary_writer: Summary writer para datos de entrenamiento.\n",
    "          X_train: Datos de entrenamiento\n",
    "        \"\"\"\n",
    "        logdir = self.logdir + '/graph'\n",
    "        tf.summary.trace_on(graph=True, profiler=False)\n",
    "        predicted_proba = self.prediction_step(X_train)\n",
    "        with train_summary_writer.as_default():\n",
    "            tf.summary.trace_export(\n",
    "                name=\"prediction_step\",\n",
    "                step=0,\n",
    "                profiler_outdir=logdir)\n",
    "            tf.summary.trace_off()\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        \"\"\"Retorna las probabilidades de clase para los datos de entrada.\n",
    "              \n",
    "        Args:\n",
    "            X: datos de entrada\n",
    "        \n",
    "        Returns:\n",
    "            predicted_proba.numpy(): Arreglo numpy de las probabilidades de \n",
    "              cada clase, para cada muestra de X.\n",
    "        \"\"\"\n",
    "        # Obtener las probabilidades de salida de cada clase\n",
    "        predicted_proba = self.prediction_step(X)\n",
    "        return predicted_proba.numpy()\n",
    "    \n",
    "    def predict_label(self, X):\n",
    "        \"\"\"Retorna la etiqueta predicha para los datos de entrada.\n",
    "        \n",
    "        Args:\n",
    "            X: datos de entrada\n",
    "        \n",
    "        Returns:\n",
    "            predicted_labels: Arreglo numpy de las predicciones de cada muestra de X.\n",
    "        \"\"\"\n",
    "        # Obtener la probabilidad de cada clase\n",
    "        predicted_proba = self.prediction_step(X)\n",
    "        # Etiquetar segun la etiqueta mas probable\n",
    "        predicted_labels = np.argmax(predicted_proba, axis=1)\n",
    "        return predicted_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5HUxx8YiRbDl"
   },
   "source": [
    "## Entrenamiento de MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "aYshAaUvRaZa"
   },
   "outputs": [],
   "source": [
    "# ----- Directorio para logs\n",
    "experiment_name = \"experiment_1\"\n",
    "\n",
    "# --- NO TOCAR\n",
    "logdir_father = \"./tarea_1_logs/\"\n",
    "logdir = logdir_father + experiment_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "xmNz6vshn1q2",
    "outputId": "71f464ee-7d8a-4c99-e975-8a05992a97f0",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "run_n_times = 5\n",
    "stats_history = []\n",
    "for run in range(run_n_times):\n",
    "    # ----- Creacion de MLP\n",
    "    mlp = MLPClassifier(\n",
    "        n_features=28*28,\n",
    "        layer_sizes=[25, 2],\n",
    "        loss_function_name='cross_entropy',\n",
    "        learning_rate=0.1,\n",
    "        batch_size=32,\n",
    "        max_epochs=100,\n",
    "        early_stopping=15,\n",
    "        logdir=logdir+'/run_%d' % run)\n",
    "\n",
    "    # ----- Entrenamiento de MLP\n",
    "    train_stats = mlp.fit(training_images, training_labels, validation_images, validation_labels)\n",
    "    stats_history.append(train_stats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uszFkplq3avX"
   },
   "source": [
    "### Tasa de acierto en validación computada sobre varias ejecuciones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 86
    },
    "colab_type": "code",
    "id": "HNS4VAkflQvE",
    "outputId": "51adcf4c-1285-4a67-c66b-ac27a5d77a58"
   },
   "outputs": [],
   "source": [
    "val_acc_history = np.array([run['val_acc_history'][-1] for run in stats_history])\n",
    "val_acc_mean = val_acc_history.mean()\n",
    "val_acc_std = val_acc_history.std()\n",
    "print('Validation accuracy %.3f +/- %.3f' % (val_acc_mean, val_acc_std))\n",
    "print(val_acc_history)\n",
    "\n",
    "train_acc_history = np.array([run['train_acc_history'][-1] for run in stats_history])\n",
    "train_acc_mean = train_acc_history.mean()\n",
    "train_acc_std = train_acc_history.std()\n",
    "print('Train accuracy %.3f +/- %.3f' % (train_acc_mean, train_acc_std))\n",
    "print(train_acc_history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tDrpTQWXa19D"
   },
   "source": [
    "## Visualización de Resultados"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8ypv7zhabDJT"
   },
   "source": [
    "### Algunas estadísticas del entrenamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 367
    },
    "colab_type": "code",
    "id": "XsDDF3BoboJS",
    "outputId": "d8d6b2bc-0e16-4fb8-8e5a-1cf966c89532"
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 2, figsize=(13, 5))\n",
    "fig.set_facecolor('white')\n",
    "\n",
    "ax[0].plot(train_stats['iteration_history'], train_stats['val_loss_history'], label='validation')\n",
    "ax[0].plot(train_stats['iteration_history'], train_stats['train_loss_history'], label='training')\n",
    "ax[0].set_xlabel('Iteration')\n",
    "ax[0].set_ylabel('Loss')\n",
    "ax[0].set_title('Loss evolution during training')\n",
    "ax[0].legend()\n",
    "\n",
    "ax[1].plot(train_stats['iteration_history'], train_stats['val_acc_history'], label='validation')\n",
    "ax[1].plot(train_stats['iteration_history'], train_stats['train_acc_history'], label='training')\n",
    "ax[1].set_xlabel('Iteration')\n",
    "ax[1].set_ylabel('Accuracy')\n",
    "ax[1].set_title('Accuracy evolution during training')\n",
    "ax[1].legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PF87rmUxbMtW"
   },
   "source": [
    "### Estadísticas del desempeño final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "XKM3ashMUVZa",
    "outputId": "1c7647c6-f86b-41d3-8fc8-2d3c866312d8"
   },
   "outputs": [],
   "source": [
    "def confusion_matrix(labels, predictions):\n",
    "    \"\"\"Calcula la matriz de confusion.\n",
    "    \n",
    "    Args:\n",
    "        labels: Array binario 1-D con las etiquetas reales.\n",
    "        predictions: Array binario 1-D con las predicciones.\n",
    "        \n",
    "    Returns:\n",
    "        TP: Numero de verdaderos positivos.\n",
    "        FP: Numero de falsos positivos.\n",
    "        FN: Numero de falsos negativos.\n",
    "        TN: Numero de verdaderos negativos.\n",
    "    \"\"\"\n",
    "    # Map labels and predictions to {0, 1, 2, 3}\n",
    "    encoded_data = 2 * labels + predictions  \n",
    "    TN = np.sum(encoded_data == 0)  # True negatives\n",
    "    FP = np.sum(encoded_data == 1)  # False positives\n",
    "    FN = np.sum(encoded_data == 2)  # False negatives\n",
    "    TP = np.sum(encoded_data == 3)  # True positives\n",
    "    return TP, FP, FN, TN\n",
    "\n",
    "\n",
    "def performance_metrics(TP, FP, FN, TN):\n",
    "    \"\"\"Calcula metricas de desempeño.\n",
    "    \n",
    "    Args:\n",
    "        TP: Numero de verdaderos positivos.\n",
    "        FP: Numero de falsos positivos.\n",
    "        FN: Numero de falsos negativos.\n",
    "        TN: Numero de verdaderos negativos.\n",
    "    \n",
    "    Returns:\n",
    "        accuracy: Porcentaje de clasificaciones correctas del detector.\n",
    "        precision: Precision del detector.\n",
    "        recall: Recall/Sensibilidad del detector.\n",
    "    \"\"\"\n",
    "    accuracy = 100.0 * (TP + TN) / (TP + TN + FP + FN)\n",
    "    precision = 100.0 * TP / (TP + FP)\n",
    "    recall = 100.0 * TP / (TP + FN)\n",
    "    print('TP: %d, TN: %d, FP: %d, FN: %d' %(TP,TN,FP,FN))\n",
    "    print('%1.4f%% Accuracy (Porcentaje de clasificaciones correctas)' % (accuracy))\n",
    "    print('%1.4f%% Precision' % (precision))\n",
    "    print('%1.4f%% Recall' % (recall))\n",
    "    print('')\n",
    "    return accuracy, precision, recall\n",
    "\n",
    "\n",
    "def roc_curve(labels, probabilities):\n",
    "    \"\"\"Calcula la curva ROC.\n",
    "    \n",
    "    Args:\n",
    "        labels: Array binario 1-D con las etiquetas reales.\n",
    "        probabilities: Array 1-D continuo en el rango [0, 1] con las\n",
    "            probabilidades de la clase 1.\n",
    "        \n",
    "    Returns:\n",
    "        tpr: Array 1-D con los valores de Tasa de Verdaderos Positivos (TPR).\n",
    "        fpr: Array 1-D con los valores de Tasa de Falsos Positivos (FPR).\n",
    "    \"\"\"\n",
    "    tpr = []\n",
    "    fpr = []\n",
    "    for threshold in np.linspace(0, 1, 1000):\n",
    "        probabilities_with_threshold = (probabilities > threshold).astype(np.float)\n",
    "        TP, FP, FN, TN = confusion_matrix(\n",
    "            labels, \n",
    "            probabilities_with_threshold)\n",
    "        tpr.append(TP/(TP+FN))\n",
    "        fpr.append(FP/(FP+TN))\n",
    "    return np.array(tpr), np.array(fpr)\n",
    "  \n",
    "\n",
    "def detection_performance_given_threshold(true_labels, prediction, threshold):\n",
    "    probabilities_with_threshold = (prediction > threshold).astype(np.float)\n",
    "    TP, FP, FN, TN = confusion_matrix(\n",
    "        true_labels, \n",
    "        probabilities_with_threshold)    \n",
    "    return TP, FP, FN, TN       \n",
    "\n",
    "  \n",
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "\n",
    "    print(cm)\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], fmt),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    plt.tight_layout()\n",
    "    \n",
    "\n",
    "predicted_train_labels = mlp.predict_label(training_images)\n",
    "predicted_val_labels = mlp.predict_label(validation_images)\n",
    "predicted_test_labels = mlp.predict_label(testing_images)\n",
    "\n",
    "cnf_matrix = sk_conf_mat(testing_labels, predicted_test_labels)\n",
    "\n",
    "# Plot non-normalized confusion matrix\n",
    "plt.figure()\n",
    "plt.grid('off')\n",
    "plt.gcf().set_facecolor('white')\n",
    "plot_confusion_matrix(cnf_matrix, classes=['%d' % chosen_digit, 'no %d' % chosen_digit],\n",
    "                      title='Confusion matrix')\n",
    "\n",
    "\n",
    "print('Training results:')\n",
    "TP, FP, FN, TN = confusion_matrix(training_labels, predicted_train_labels)\n",
    "accuracy, precision, recall = performance_metrics(TP, FP, FN, TN)\n",
    "\n",
    "print('Validation results:')\n",
    "TP, FP, FN, TN = confusion_matrix(validation_labels, predicted_val_labels)\n",
    "accuracy, precision, recall = performance_metrics(TP, FP, FN, TN)\n",
    "\n",
    "print('Test results:')\n",
    "TP, FP, FN, TN = confusion_matrix(testing_labels, predicted_test_labels)\n",
    "accuracy, precision, recall = performance_metrics(TP, FP, FN, TN)\n",
    "\n",
    "predicted_test_proba = mlp.predict_proba(testing_images)\n",
    "tpr, fpr = roc_curve(testing_labels, predicted_test_proba[:, 1])\n",
    "\n",
    "fig, ax = plt.subplots(1, 2, figsize=(13, 5))\n",
    "fig.set_facecolor('white')\n",
    "\n",
    "ax[0].set_title('ROC Curve')\n",
    "ax[0].plot(fpr, tpr, linewidth=2, alpha=0.5)\n",
    "ax[0].set_xlabel('False Positive Rate')\n",
    "ax[0].set_ylabel('True Positive Rate')\n",
    "\n",
    "ax[1].set_title('DET Curve')\n",
    "ax[1].plot(fpr, 1.0 - tpr, linewidth=2, alpha=0.5)\n",
    "ax[1].set_xlabel('False Positive Rate')\n",
    "ax[1].set_ylabel('False Negative Rate')\n",
    "ax[1].set_yscale('log')\n",
    "ax[1].set_xscale('log')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lsWfO5koe6wg"
   },
   "source": [
    "### Performance según umbral de clasificación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "neruAyQbecmn",
    "outputId": "ce4a4256-5e2f-457a-ea28-48abc498fab7"
   },
   "outputs": [],
   "source": [
    "predicted_validation_proba = mlp.predict_proba(validation_images)\n",
    "TP, FP, FN, TN = detection_performance_given_threshold(validation_labels, predicted_validation_proba[:, 1], threshold=0.5)\n",
    "print('TP: %d, TN: %d, FP: %d, FN: %d' %(TP,TN,FP,FN))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "X2zFsvMobYvZ"
   },
   "source": [
    "### Visualización de clasificaciones en el test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 810
    },
    "colab_type": "code",
    "id": "KyzPqoEgV0ky",
    "outputId": "236b21ed-b683-4aac-ff31-b7d0ffd2ff71"
   },
   "outputs": [],
   "source": [
    "def show_classifications(images, labels, probabilities, result_type='TP'):\n",
    "    \"\"\" Muestra ejemplos de imagenes para tipos de errores.\n",
    "    \n",
    "    Args:\n",
    "        images: Array de dimensiones (n_ejemplos, n_pixeles) con imagenes.\n",
    "        labels: Array de dimensiones (n_ejemplos,) con las etiquetas reales.\n",
    "        probabilities: Array de dimensiones (n_ejemplos,) con las probabilidades\n",
    "            de la clase 1.\n",
    "        result_type: 'TP', 'FP', 'FN', o 'TP', tipo de error a mostrar.\n",
    "    \"\"\"\n",
    "    dict_types = {'TN': 0, 'FP': 1, 'FN': 2, 'TP': 3}\n",
    "    predictions = (probabilities > 0.5).astype(np.int32)\n",
    "    encoded_data = 2 * labels + predictions \n",
    "    useful = np.where(encoded_data == dict_types[result_type])[0]\n",
    "    size = min(4, useful.shape[0])\n",
    "    chosen = np.random.choice(useful, size=size, replace=False)\n",
    "    fig, ax = plt.subplots(1, 4, figsize=(10,4))\n",
    "    fig.set_facecolor('white')\n",
    "    for i, idx in enumerate(chosen):\n",
    "        image = images[idx, :]\n",
    "        digit = labels[idx]\n",
    "        predicted_label = predictions[idx]\n",
    "        proba = probabilities[idx]\n",
    "        ax[i].imshow(image.reshape((28, 28)))\n",
    "        ax[i].set_title(\"True Class: %d\\nPredicted Class: %d\\nProb. of Class 1: %1.4f\"\n",
    "                        % (digit, predicted_label, proba))\n",
    "        ax[i].axis('off')\n",
    "    for j in range(i+1, 4):\n",
    "        ax[j].axis('off')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "predicted_test_proba = mlp.predict_proba(testing_images)\n",
    "predicted_test_proba = predicted_test_proba[:, 1]\n",
    "\n",
    "print('True Positives:')\n",
    "show_classifications(testing_images, testing_labels, predicted_test_proba , result_type='TP')\n",
    "\n",
    "print('True Negatives:')\n",
    "show_classifications(testing_images, testing_labels, predicted_test_proba , result_type='TN')\n",
    "\n",
    "print('False Positive:')\n",
    "show_classifications(testing_images, testing_labels, predicted_test_proba , result_type='FP')\n",
    "\n",
    "print('False Negative:')\n",
    "show_classifications(testing_images, testing_labels, predicted_test_proba , result_type='FN')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7b6uAwj7q2TA"
   },
   "source": [
    "Si quieren borrar su log_dir, utilicen lo siguiente:\n",
    "\n",
    "Ejecutar comandos en bash\n",
    "* %%bash\n",
    "\n",
    "para mirar lo que hay en el directorio actual:\n",
    "* ls \n",
    "\n",
    "para borrar la carpeta \"tarea_1_logs\"\n",
    "* rm -r tarea_1_logs\n",
    "\n",
    "buscar procesos asociados a tensorboard\n",
    "* ps aux | grep tensorboard\n",
    "\n",
    "terminar un proceso\n",
    "* kill -9 process_id (process_id es el pid del proceso)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "colab_type": "code",
    "id": "WDbgq2lP4xZy",
    "outputId": "01b03a30-f2f8-4e00-9a60-0b008a1d4383"
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "ls tarea_1_logs/exp_xentropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "veKtB6KRkLIu"
   },
   "outputs": [],
   "source": [
    "#!rm -r tarea_1_logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "JviRuQufPeZf",
    "outputId": "9c280763-b2af-420a-bea0-5d640a86a54d"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Q4cnGTMyKsdW"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Tarea1_Notebook.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
