{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9hI8tV5uVHZX"
   },
   "source": [
    "### [Google Colab] subir código cifar10.py. Basta con ejecutar una vez (inmune a reinicios de kernel de python, no de la máquina)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 46,
     "resources": {
      "http://localhost:8080/nbextensions/google.colab/files.js": {
       "data": "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",
       "headers": [
        [
         "content-type",
         "application/javascript"
        ]
       ],
       "ok": true,
       "status": 200,
       "status_text": "OK"
      }
     }
    },
    "colab_type": "code",
    "id": "x7EKoANMPBps",
    "outputId": "346b460f-4c6c-475f-bc71-28ba6573b02f"
   },
   "outputs": [],
   "source": [
    "from google.colab import files\n",
    "\n",
    "uploaded = files.upload()\n",
    "\n",
    "for fn in uploaded.keys():\n",
    "    print('User uploaded file \"{name}\" with length {length} bytes'.format(\n",
    "        name=fn, length=len(uploaded[fn])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pgIK9vPiSHU1"
   },
   "source": [
    "# Aquí comienza el cuerpo del código\n",
    "## Reinicie luego de entrenar cada modelo. Recuerde actualizar el directorio de tensorboard."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vFrRwKUHOhs-"
   },
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "from cifar10 import CIFAR10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "EYzZFJpM08iZ"
   },
   "source": [
    "### Constructor del dataset. Modifique aquí el *flag* de *data augmentation*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1Hk74ROkOhtO"
   },
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "batch_size = 64\n",
    "cifar10 = CIFAR10(batch_size=batch_size, validation_proportion=0.1, augment_data=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HCAGDv5M0hFU"
   },
   "source": [
    "## Elija aquí el directorio donde guardará los registros de Tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "yqn2fJpRUYkr"
   },
   "outputs": [],
   "source": [
    "PARENT_DIR = './summaries/'\n",
    "SUMMARIES_DIR = PARENT_DIR + 'mlp'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "AaGDU7CF0p71"
   },
   "source": [
    "## Constructores de modelos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fTXZBRJCswCE"
   },
   "outputs": [],
   "source": [
    "# Modelo de red convolucional\n",
    "class ConvClassifier(tf.keras.Model):\n",
    "    \"\"\"Implementacion de clasificador en base a red neuronal convolucional.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        input_shape,\n",
    "        layer_sizes,\n",
    "        learning_rate=0.0005,\n",
    "        dropout_rate=0.5):\n",
    "        \"\"\"Construye un clasificador Red Neuronal Convolucional.\n",
    "        \n",
    "        Args:\n",
    "            input_shape: tupla que contiene dimensiones de las imagenes de entrada\n",
    "                al modelo, exluyendo la dimensióón de batch. La tupla ha de tener largo 3\n",
    "                y formato (Image_height, image_width, Channels)\n",
    "            layer_sizes: Lista de enteros que indica la cantidad de filtros \n",
    "                convolucionales en cada capa de la red.\n",
    "            learning_rate: Escalar que indica la tasa de aprendizaje.\n",
    "            dropout_rate: Probabilidad con la que se apaga cada neurona de la \n",
    "                capa fully connected 1 durante el entrenamiento del modelo.\n",
    "        \"\"\"\n",
    "        \n",
    "        # Inicializa atributos propios de un objeto del tipo tf.keras.Model\n",
    "        super().__init__()\n",
    "        # Agregar parametros al objeto\n",
    "        self.input_shape_tuple = input_shape\n",
    "        self.layer_sizes = layer_sizes\n",
    "        self.learning_rate = learning_rate\n",
    "        self.dropout_rate = dropout_rate\n",
    "        # Inicializa capas del modelo\n",
    "        self.layers_list = self._init_layers(layer_sizes)\n",
    "        # Crea un objeto optimizador\n",
    "        self.optimizer = tf.keras.optimizers.RMSprop(learning_rate=self.learning_rate)\n",
    "        # Crea un objeto funcion de costo-perdida\n",
    "        self.loss_object = tf.keras.losses.CategoricalCrossentropy(from_logits=True)\n",
    "        self.loss_function_name = 'xentropy'\n",
    "\n",
    "    def _init_layers(self, layer_sizes):\n",
    "        \"\"\"Inicializa-crea los objetos de cada capa de la red CNN.\n",
    "\n",
    "        Args:\n",
    "        layer_sizes: Lista de enteros que indica la cantidad de filtros convolucionales\n",
    "        de cada capa convolucional.\n",
    "\n",
    "        Returns:\n",
    "        layers_list: Lista de objetos asociados a cada capa creada.\n",
    "        \"\"\"\n",
    "        n_layers = len(layer_sizes)\n",
    "        layers_list = []\n",
    "        # Inicializacion de capas del modelo\n",
    "        # Inicializacion de capas de entrada, que fija el tamaño de las entradas\n",
    "        layers_list.append(tf.keras.layers.InputLayer((self.input_shape_tuple)))\n",
    "        # Inicializacion de capas convolucionales\n",
    "        for i in range(n_layers):\n",
    "            layers_list.append(\n",
    "                tf.keras.layers.Conv2D(\n",
    "                    layer_sizes[i], kernel_size=5, padding='same', \n",
    "                    bias_initializer=tf.keras.initializers.Constant(value=0.05), \n",
    "                    activation=tf.nn.relu, name='conv%i' % (i+1)))\n",
    "            # Inicializacion de funcion de capa de pooling\n",
    "            layers_list.append(\n",
    "                tf.keras.layers.MaxPool2D())\n",
    "        # Inicializacion de capa que aplana featuremaps de la convolucion para que \n",
    "        # puedan entrar a capas fully-connected\n",
    "        layers_list.append(\n",
    "            tf.keras.layers.Flatten())\n",
    "        # Inicializacion de capa fully-connected 1\n",
    "        layers_list.append(\n",
    "            tf.keras.layers.Dense(50, activation=tf.nn.relu, name='fc1'))\n",
    "        # Inicializacion de capa dropout\n",
    "        layers_list.append(\n",
    "            tf.keras.layers.Dropout(self.dropout_rate))\n",
    "        # Inicializacion de capa fully-connected 2\n",
    "        layers_list.append(\n",
    "            tf.keras.layers.Dense(10, name='fc2'))\n",
    "        # Inicializacion de activacion de salida\n",
    "        layers_list.append(\n",
    "            tf.keras.layers.Activation(tf.nn.softmax))\n",
    "        return layers_list\n",
    "\n",
    "    def call(self, x, training=False, get_logits=False):\n",
    "        \"\"\"Metodo que entrega la salida del modelo, \n",
    "        define el paso forward de la red neuronal, relacionando las capas \n",
    "        de la red entre si. Se define la arquitectura del modelo.\n",
    "\n",
    "        Args:\n",
    "          x: Tensor de entrada de dimensiones (batch_size, n_features).\n",
    "          training: Boolean que indica si se esta en fase de entrenamiento o no.\n",
    "            En este caso define el comportamiento durante entrenamiento y evaluacion de capas \n",
    "             como dropout o batch normalization \n",
    "          get_logits: Bolean que indica si como salida del modelo se obtienen los\n",
    "            logits o las predicciones\n",
    "\n",
    "        Returns:\n",
    "          x: Tensor de salida de la red, de dimensiones (batch_size, n_classes).\n",
    "        \"\"\"\n",
    "        for layer_index, layer in enumerate(self.layers_list):\n",
    "            # se se desean los logits, se retorna la salida del modelo sin la ultima activacion\n",
    "            if get_logits and layer_index == (len(self.layers_list)-1):\n",
    "                return x \n",
    "            x = layer(x, training=training)\n",
    "        return x\n",
    "\n",
    "    @tf.function\n",
    "    def train_step(self, x_data, y_labels):\n",
    "        \"\"\"Metodo que realiza la actualizacion por gradiente.\n",
    "\n",
    "        Se aplica un algoritmo de optimizacion para ejecutar una\n",
    "        iteracion de minimizacion por gradiente sobre el loss del modelo.\n",
    "\n",
    "        Args:\n",
    "        x_data: datos de entrenamientos sobre los que calcular la loss.\n",
    "        y_labels: etiquetas de entrenamiento sobre las que calcular la loss.\n",
    "\n",
    "        Returns:\n",
    "        loss: el valor de la funcion de costo para x_data, y_labels.\n",
    "        accuracy: porcentaje de aciertos entre y_labels y las predicciones sobre x_data.\n",
    "        \"\"\"\n",
    "        with tf.GradientTape() as tape:\n",
    "        # training=True is only needed if there are layers with different\n",
    "        # behavior during training versus inference (e.g. Dropout).\n",
    "            logits = self.call(x_data, training=True, get_logits=True)\n",
    "            loss = self.loss_object(y_labels, logits)\n",
    "        gradients = tape.gradient(loss, self.trainable_variables)\n",
    "        self.optimizer.apply_gradients(zip(gradients, self.trainable_variables))\n",
    "        return loss\n",
    "\n",
    "    def _get_accuracy_from_logits(self, logits, labels):\n",
    "        \"\"\"Metodo que entrega el accuracy entre logits de prediccion y etiquetas.\n",
    "\n",
    "        Args:\n",
    "          logits: para datos de etiqueta labels.\n",
    "          labels: etiquetas.\n",
    "\n",
    "        Returns:\n",
    "          accuracy: porcentaje de aciertos por los logits, respecto a los labels.\n",
    "        \"\"\"\n",
    "        softmax_prediction = tf.nn.softmax(logits)\n",
    "        correct_prediction = tf.equal(tf.argmax(softmax_prediction, 1),\n",
    "                             tf.argmax(labels, 1))\n",
    "        accuracy = tf.reduce_mean(\n",
    "            tf.cast(correct_prediction, tf.float32),\n",
    "            name='accuracy')\n",
    "        return accuracy\n",
    "\n",
    "    @tf.function\n",
    "    def eval_step(self, x_data, y_labels):\n",
    "        \"\"\"Metodo que evalua la funcion de costo y el accuracy del modelo.\n",
    "\n",
    "        Args:\n",
    "        x_data: datos sobre los que calcular la loss y accuracy.\n",
    "        y_labels: etiquetas sobre las que calcular la loss y accuracy.\n",
    "\n",
    "        Returns:\n",
    "        loss: el valor de la funcion de costo para x_data, y_labels.\n",
    "        accuracy: porcentaje de aciertos entre y_labels y las predicciones sobre x_data.\n",
    "        \"\"\"\n",
    "        logits = self.call(x_data, training=False, get_logits=True)\n",
    "        loss = self.loss_object(y_labels, logits)\n",
    "        accuracy = self._get_accuracy_from_logits(logits, y_labels)\n",
    "        return loss, accuracy\n",
    "\n",
    "    @tf.function\n",
    "    def prediction_step(self, x_data):\n",
    "        \"\"\"Metodo que retorna predicciones para x_data\n",
    "\n",
    "        Args:\n",
    "          x_data: datos a predecir.\n",
    "\n",
    "        Returns:\n",
    "          predictions: predicciones para x_data\n",
    "        \"\"\"\n",
    "        predictions = self.call(x_data, training=False, get_logits=False)\n",
    "        return predictions\n",
    "\n",
    "    def write_to_evaluation_summary(self, summary_writer, step, loss, accuracy):\n",
    "        \"\"\"Metodo que calcula metricas y parametros a ser monitoreados en tensorboard.\n",
    "        Monitorea histograma de parametros, metricas del modelo e imagenes de filtros\n",
    "        de primera capa convolucional. Para datos de evaluacion\n",
    "\n",
    "        Args:\n",
    "          summary_writer: Summary writer de evaluacion\n",
    "          step: iteracion del evaluacion\n",
    "          loss: valor de la funcion de costo para datos de evaluacion\n",
    "          accuracy: accuracy para datos de evaluacion\n",
    "        \"\"\"\n",
    "        with summary_writer.as_default():\n",
    "            for layer_i in self.layers_list:\n",
    "                layer_name = layer_i.name\n",
    "                if 'fc' in layer_name or 'conv' in layer_name:\n",
    "                    weights, biases = layer_i.get_weights()\n",
    "                    tf.summary.histogram(layer_name + '/weights', weights, step=step)\n",
    "                    tf.summary.histogram(layer_name + '/biases', biases, step=step)\n",
    "                if 'conv1' in layer_name:\n",
    "                    tf.summary.image(\n",
    "                        'conv1_filters',\n",
    "                        tf.transpose(weights, perm=[3, 0, 1, 2]), step=step,\n",
    "                        max_outputs=self.layer_sizes[0]\n",
    "                    )\n",
    "            tf.summary.scalar(self.loss_function_name + '_loss', loss, step=step)  \n",
    "            tf.summary.scalar('accuracy', accuracy, step=step)  \n",
    "\n",
    "    def write_to_learning_summary(self, summary_writer, step, loss, accuracy):\n",
    "        \"\"\"Metodo que calcula metricas a ser monitoreados en tensorboard.\n",
    "        Monitorea metricas del modelo.\n",
    "\n",
    "        Args:\n",
    "          summary_writer: Summary writer de\n",
    "          step: iteracion\n",
    "          loss: valor de la funcion de costo para datos\n",
    "          accuracy: accuracy para datos\n",
    "        \"\"\"\n",
    "        with summary_writer.as_default():\n",
    "            tf.summary.scalar(self.loss_function_name + '_loss', loss, step=step)  \n",
    "            tf.summary.scalar('accuracy', accuracy, step=step)  \n",
    "    \n",
    "\n",
    "    def write_graph_to_summary(self, summary_writer, X_data, logdir):\n",
    "        \"\"\"Metodo que guarda el grafo de evaluacion (arquitectura del modelo)\n",
    "        en tensorboard. Se guardan las operaciones involugradas en un paso forward\n",
    "        o self.call del modelo\n",
    "\n",
    "        Args:\n",
    "          summary_writer: Summary writer para datos\n",
    "          X_data: Datos\n",
    "        \"\"\"\n",
    "        logdir = logdir + '/graph'\n",
    "        tf.summary.trace_on(graph=True, profiler=False)\n",
    "        predicted_proba = self.prediction_step(X_data)\n",
    "        with summary_writer.as_default():\n",
    "            tf.summary.trace_export(\n",
    "                name=\"prediction_step\",\n",
    "                step=0,\n",
    "                profiler_outdir=logdir)\n",
    "        tf.summary.trace_off()\n",
    "    \n",
    "    \n",
    "    def predict_proba(self, X):\n",
    "        \"\"\"Retorna las probabilidades de clase para los datos de entrada.\n",
    "              \n",
    "        Args:\n",
    "            X: datos de entrada\n",
    "        \n",
    "        Returns:\n",
    "            predicted_proba.numpy(): Arreglo numpy de las probabilidades de \n",
    "              cada clase, para cada muestra de X.\n",
    "        \"\"\"\n",
    "        # Obtener las probabilidades de salida de cada clase\n",
    "        predicted_proba = self.prediction_step(X)\n",
    "        return predicted_proba.numpy()\n",
    "    \n",
    "    def predict_label(self, X):\n",
    "        \"\"\"Retorna la etiqueta predicha para los datos de entrada.\n",
    "        \n",
    "        Args:\n",
    "            X: datos de entrada\n",
    "        \n",
    "        Returns:\n",
    "            predicted_labels: Arreglo numpy de las predicciones de cada muestra de X.\n",
    "        \"\"\"\n",
    "        # Obtener la probabilidad de cada clase\n",
    "        predicted_proba = self.prediction_step(X)\n",
    "        # Etiquetar segun la etiqueta mas probable\n",
    "        predicted_labels = np.argmax(predicted_proba, axis=1)\n",
    "        return predicted_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QBFYJU6uPGLB"
   },
   "outputs": [],
   "source": [
    "# Modelo de red MLP\n",
    "class MLPClassifier(ConvClassifier):\n",
    "    \"\"\"Implementacion de clasificador en base a red MLP.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        learning_rate=0.0005,\n",
    "        dropout_rate=0.5):\n",
    "        \"\"\"Construye un clasificador Perceptron Multicapa de 2 capas.\n",
    "        \n",
    "        Args:\n",
    "            learning_rate: Escalar que indica la tasa de aprendizaje.\n",
    "            dropout_rate: Probabilidad con la que se apaga cada neurona de la \n",
    "                capa fully connected 1 durante el entrenamiento del modelo.\n",
    "        \"\"\"\n",
    "        \n",
    "        # Inicializa atributos propios de un objeto del tipo tf.keras.Model\n",
    "        tf.keras.Model.__init__(self)\n",
    "        # Agregar parametros al objeto\n",
    "        self.learning_rate = learning_rate\n",
    "        self.dropout_rate = dropout_rate\n",
    "        # Inicializa capas del modelo\n",
    "        self.layers_list = self._init_layers()\n",
    "        # Crea un objeto optimizador\n",
    "        self.optimizer = tf.keras.optimizers.RMSprop(learning_rate=self.learning_rate)\n",
    "        # Crea un objeto funcion de costo-perdida\n",
    "        self.loss_object = tf.keras.losses.CategoricalCrossentropy(from_logits=True)\n",
    "        self.loss_function_name = 'xentropy'\n",
    "    \n",
    "    def _init_layers(self):\n",
    "        \"\"\"Inicializa-crea los objetos de cada capa de la red MLP.\n",
    "\n",
    "        Returns:\n",
    "          layers_list: Lista de objetos asociados a cada capa creada.\n",
    "        \"\"\"\n",
    "        layers_list = []\n",
    "        # Inicializacion de capas del modelo\n",
    "        # Inicializacion de capas de entrada, que fija el tamaño de las entradas\n",
    "        layers_list.append(tf.keras.layers.Flatten())\n",
    "        # Inicializacion de capas convolucionales\n",
    "        layers_list.append(tf.keras.layers.Dense(100, activation=tf.nn.relu, name='fc1'))\n",
    "        # Inicializacion de capa dropout\n",
    "        layers_list.append(tf.keras.layers.Dropout(self.dropout_rate))\n",
    "        # Inicializacion de capa fully-connected 2\n",
    "        layers_list.append(tf.keras.layers.Dense(10, name='fc2'))\n",
    "        # Inicializacion de activacion de salida\n",
    "        layers_list.append(tf.keras.layers.Activation(tf.nn.softmax))\n",
    "        return layers_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vvYNrw25Vi__"
   },
   "source": [
    "## Inicializacion de modelos\n",
    "\n",
    "### Aqui puede escoger que modelo usar y modificar el valor de *dropout_rate* usado al computar *train_step* para activar/desactivar el *dropout_rate*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_Y0ezm__OhtY"
   },
   "outputs": [],
   "source": [
    "#### PARAMETROS DE MODELOS A MODIFICAR ###\n",
    "use_convnet = True\n",
    "n_filters_convs = [16, 32, 64]\n",
    "dropout_rate = 0.5\n",
    "\n",
    "if use_convnet:\n",
    "    model = ConvClassifier(\n",
    "        input_shape=(32, 32, 3),\n",
    "        layer_sizes=n_filters_convs,\n",
    "        dropout_rate=dropout_rate,\n",
    "        learning_rate=0.0005)\n",
    "\n",
    "else:\n",
    "    model = MLPClassifier(\n",
    "        dropout_rate=dropout_rate,\n",
    "        learning_rate=0.0005)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DAIrQw7r2E47"
   },
   "source": [
    "### Construcción del optimizador + funciones auxiliares"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 89
    },
    "colab_type": "code",
    "id": "wnMUAQYlOhtc",
    "outputId": "e551a17a-b07c-43db-c140-d5afb95d761b"
   },
   "outputs": [],
   "source": [
    "# Useful training functions\n",
    "def validate(model, val_summary_writer, step):\n",
    "    cifar10.shuffleValidation()\n",
    "    batches = cifar10.getValidationSet(asBatches=True)\n",
    "    accs = []\n",
    "    xent_vals = []\n",
    "    for batch in batches:\n",
    "        data, labels = batch\n",
    "        xentropy_val, acc = model.eval_step(data, labels)\n",
    "        accs.append(acc)\n",
    "        xent_vals.append(xentropy_val)\n",
    "    mean_xent = np.array(xent_vals).mean()    \n",
    "    mean_acc = np.array(accs).mean()\n",
    "    model.write_to_evaluation_summary(val_summary_writer, step, mean_xent, mean_acc)\n",
    "    return mean_acc, mean_xent\n",
    "\n",
    "def test(model):\n",
    "    batches = cifar10.getTestSet(asBatches=True)\n",
    "    accs = []\n",
    "    for batch in batches:\n",
    "        data, labels = batch\n",
    "        _, acc = model.eval_step(data, labels)\n",
    "        accs.append(acc)\n",
    "    mean_acc = np.array(accs).mean()\n",
    "    return mean_acc\n",
    "\n",
    "# Tensorboard writers\n",
    "train_writer = tf.summary.create_file_writer(SUMMARIES_DIR+'/train')\n",
    "model.write_graph_to_summary(train_writer, cifar10.getValidationSet()[0], SUMMARIES_DIR)\n",
    "validation_writer = tf.summary.create_file_writer(SUMMARIES_DIR+'/validation')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UUc5-KOHSALR"
   },
   "source": [
    "## Entrenar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "TRKMxMztOhtg",
    "outputId": "32a76ab4-d784-492f-cb91-42b2f7aaf042",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "cifar10.reset()\n",
    "print(\"Trainable variables\")\n",
    "for n in model.trainable_weights:\n",
    "    print(n.name, ' ', n.shape)\n",
    "if use_convnet:\n",
    "    epochs = 30\n",
    "else:\n",
    "    epochs = 50\n",
    "    \n",
    "t_i = time.time()\n",
    "n_batches = cifar10.n_batches\n",
    "val_acc_vals = []\n",
    "test_acc_vals = []\n",
    "while cifar10.getEpoch() < epochs:\n",
    "    epoch = cifar10.getEpoch()\n",
    "    batch, batch_idx = cifar10.nextBatch()\n",
    "    batch_data = batch[0]\n",
    "    batch_labels = batch[1]\n",
    "    \n",
    "    # just a training iteration\n",
    "    model.train_step(batch_data, batch_labels)\n",
    "    \n",
    "    step = batch_idx+epoch*n_batches\n",
    "    \n",
    "    # Write training summary\n",
    "    if step % 50 == 0:\n",
    "        loss, accuracy = model.eval_step(batch_data, batch_labels)\n",
    "        model.write_to_learning_summary(train_writer, step, loss, accuracy)\n",
    "        \n",
    "    # gradient (by layer) statistics over last training batch & validation summary\n",
    "    if batch_idx==0:\n",
    "        loss, acc = model.eval_step(batch_data, batch_labels)\n",
    "        validation_accuracy, validation_loss = validate(model, validation_writer, step)\n",
    "        print('[Epoch %d, it %d] Training acc. %.3f, loss %.3f. \\\n",
    "Valid. acc. %.3f, loss %.3f' % (\n",
    "            epoch,\n",
    "            step,\n",
    "            acc,\n",
    "            loss,\n",
    "            validation_accuracy,\n",
    "            validation_loss\n",
    "        ))\n",
    "        val_acc_vals.append(validation_accuracy)\n",
    "        test_accuracy = test(model)\n",
    "        test_acc_vals.append(test_accuracy)\n",
    "        print(\"Time elapsed %.2f minutes\" % ((time.time()-t_i)/60.0))\n",
    "\n",
    "val_acc_vals = np.array(val_acc_vals)\n",
    "test_acc_vals = np.array(test_acc_vals)\n",
    "best_epoch = np.argmax(val_acc_vals)\n",
    "test_acc_at_best = test_acc_vals[best_epoch]\n",
    "print('*'*30)\n",
    "print(\"Testing set accuracy @ epoch %d (best validation acc): %.4f\" % (best_epoch, test_acc_at_best))\n",
    "print('*'*30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "h554rbJcR7Jj"
   },
   "source": [
    "## Visualizar en Tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "aL3_pq6eOhtl"
   },
   "outputs": [],
   "source": [
    "LOG_DIR = PARENT_DIR  # este es el logdir de nuestros summaries\n",
    "print(\"Showing summaries at %s\" % (LOG_DIR))\n",
    "\n",
    "%load_ext tensorboard\n",
    "%tensorboard --logdir='$LOG_DIR'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CUYkbGcHT3Vz"
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "ls\n",
    "ps aux | grep -e 'tensorboard'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jRtK8EiGMJ9n"
   },
   "outputs": [],
   "source": [
    "#!rm -r summaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Oi-pkaRAN7Yf"
   },
   "outputs": [],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "convnet-tutorial.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
